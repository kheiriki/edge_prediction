{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "! pip install PyYAML\n",
    "!pip install pydantic\n",
    "!pip install rdflib\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "try:\n",
    "    import dgl\n",
    "except:\n",
    "    !pip install dgl\n",
    "    import dgl\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "from functools import partial\n",
    "\n",
    "class RGCNLayer(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat, num_rels, num_bases=-1, bias=None,\n",
    "                 activation=None, is_input_layer=False):\n",
    "        super(RGCNLayer, self).__init__()\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.is_input_layer = is_input_layer\n",
    "\n",
    "        # sanity check\n",
    "        if self.num_bases <= 0 or self.num_bases > self.num_rels:\n",
    "            self.num_bases = self.num_rels\n",
    "\n",
    "        # weight bases in equation (3)\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.num_bases, self.in_feat,\n",
    "                                                self.out_feat))\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # linear combination coefficients in equation (3)\n",
    "            self.w_comp = nn.Parameter(torch.Tensor(self.num_rels, self.num_bases))\n",
    "\n",
    "        # add bias\n",
    "        if self.bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_feat))\n",
    "\n",
    "        # init trainable parameters\n",
    "        nn.init.xavier_uniform_(self.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        if self.num_bases < self.num_rels:\n",
    "            nn.init.xavier_uniform_(self.w_comp,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "        if self.bias:\n",
    "            nn.init.xavier_uniform_(self.bias,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, g):\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # generate all weights from bases (equation (3))\n",
    "            weight = self.weight.view(self.in_feat, self.num_bases, self.out_feat)\n",
    "            weight = torch.matmul(self.w_comp, weight).view(self.num_rels,\n",
    "                                                        self.in_feat, self.out_feat)\n",
    "        else:\n",
    "            weight = self.weight\n",
    "\n",
    "        if self.is_input_layer:\n",
    "            def message_func(edges):\n",
    "                # for input layer, matrix multiply can be converted to be\n",
    "                # an embedding lookup using source node id\n",
    "                embed = weight.view(-1, self.out_feat)\n",
    "                index = edges.data['rel_type'] * self.in_feat + edges.src['id']\n",
    "                return {'msg': embed[index] * edges.data['norm']}\n",
    "        else:\n",
    "            def message_func(edges):\n",
    "                w = weight[edges.data['rel_type']]\n",
    "                msg = torch.bmm(edges.src['h'].unsqueeze(1), w).squeeze()\n",
    "                msg = msg * edges.data['norm']\n",
    "                return {'msg': msg}\n",
    "\n",
    "        def apply_func(nodes):\n",
    "            h = nodes.data['h']\n",
    "            if self.bias:\n",
    "                h = h + self.bias\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            return {'h': h}\n",
    "\n",
    "        g.update_all(message_func, fn.sum(msg='msg', out='h'), apply_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels,\n",
    "                 num_bases=-1, num_hidden_layers=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "        # create initial features\n",
    "        self.features = self.create_features()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input to hidden\n",
    "        i2h = self.build_input_layer()\n",
    "        self.layers.append(i2h)\n",
    "        # hidden to hidden\n",
    "        for _ in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer()\n",
    "            self.layers.append(h2h)\n",
    "        # hidden to output\n",
    "        h2o = self.build_output_layer()\n",
    "        self.layers.append(h2o)\n",
    "\n",
    "    # initialize feature for each node\n",
    "    def create_features(self):\n",
    "        features = torch.arange(self.num_nodes)\n",
    "        return features\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return RGCNLayer(self.num_nodes, self.h_dim, self.num_rels, self.num_bases,\n",
    "                         activation=F.relu, is_input_layer=True)\n",
    "\n",
    "    def build_hidden_layer(self):\n",
    "        return RGCNLayer(self.h_dim, self.h_dim, self.num_rels, self.num_bases,\n",
    "                         activation=F.relu)\n",
    "\n",
    "    def build_output_layer(self):\n",
    "        return RGCNLayer(self.h_dim, self.out_dim, self.num_rels, self.num_bases,\n",
    "                         activation=partial(F.softmax, dim=1))\n",
    "\n",
    "    def forward(self, g):\n",
    "        if self.features is not None:\n",
    "            g.ndata['id'] = self.features\n",
    "        for layer in self.layers:\n",
    "            layer(g)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "\n",
    "class CustomDataset(DGLDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='custom')\n",
    "    \n",
    "    def process(self):\n",
    "        # Define your graph and node/edge features here\n",
    "        g = dgl.DGLGraph()\n",
    "        g.add_nodes(10)\n",
    "        g.add_edges([0, 1, 2, 3, 4, 5, 6, 7], [1, 2, 3, 4, 5, 6, 7, 8])\n",
    "        g.ndata['feat'] = torch.randn(10, 5)\n",
    "        g.edata['feat'] = torch.randn(8, 3)\n",
    "        \n",
    "        # Save the processed graph\n",
    "        self._graph = g\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return the graph and its label\n",
    "        return self._graph, torch.tensor([0])\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return the number of graphs in the dataset\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "path=\"/home/leanna/unfollow_prediction/Markov_chain/dataset\"\n",
    "ds = dgl.data.CSVDataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=ds[0]\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y is a numpy array representing your labels\n",
    "num_nodes = len(data)\n",
    "\n",
    "# Set the percentage of data for training, e.g., 80%\n",
    "train_percentage = 0.8\n",
    "\n",
    "# Calculate the number of training samples\n",
    "num_train_samples = int(num_nodes * train_percentage)\n",
    "\n",
    "# Generate indices for all nodes\n",
    "all_indices = np.arange(num_nodes)\n",
    "\n",
    "# Shuffle the indices\n",
    "np.random.shuffle(all_indices)\n",
    "\n",
    "# Select the first num_train_samples indices as train_idx\n",
    "train_idx = all_indices[:num_train_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph data\n",
    "num_nodes = data.num_nodes\n",
    "num_rels = 1\n",
    "num_classes = 3 \n",
    "labels = 6\n",
    "train_idx = data.train_idx\n",
    "# split training and validation set\n",
    "val_idx = train_idx[:len(train_idx) // 5]\n",
    "train_idx = train_idx[len(train_idx) // 5:]\n",
    "\n",
    "# edge type and normalization factor\n",
    "edge_type = torch.from_numpy(data.edge_type)\n",
    "edge_norm = torch.from_numpy(data.edge_norm).unsqueeze(1)\n",
    "\n",
    "labels = torch.from_numpy(labels).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "n_hidden = 12 # number of hidden units\n",
    "n_bases = 3 # use number of relations as number of bases\n",
    "n_hidden_layers = 4 # use 1 input layer, 1 output layer, no hidden layer\n",
    "n_epochs = 50 # epochs to train\n",
    "lr = 0.01 # learning rate\n",
    "l2norm = 0.001 # L2 norm coefficient\n",
    "\n",
    "# create graph\n",
    "g = DGLGraph((data.edge_src, data.edge_dst))\n",
    "g.edata.update({'rel_type': edge_type, 'norm': edge_norm})\n",
    "\n",
    "# create model\n",
    "model = Model(g.num_nodes(),\n",
    "              n_hidden,\n",
    "              num_classes,\n",
    "              num_rels,\n",
    "              num_bases=n_bases,\n",
    "              num_hidden_layers=n_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "\n",
    "# initialize lists to store accuracy and loss values over epochs\n",
    "train_acc_list = []\n",
    "train_loss_list = []\n",
    "val_acc_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "print(\"start training...\")\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model.forward(g)\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx])\n",
    "    train_acc = train_acc.item() / len(train_idx)\n",
    "    train_acc_list.append(train_acc)\n",
    "    train_loss_list.append(loss.item())\n",
    "    \n",
    "    val_loss = F.cross_entropy(logits[val_idx], labels[val_idx])\n",
    "    val_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx])\n",
    "    val_acc = val_acc.item() / len(val_idx)\n",
    "    val_acc_list.append(val_acc)\n",
    "    val_loss_list.append(val_loss.item())\n",
    "    \n",
    "    print(\"Epoch {:05d} | \".format(epoch) +\n",
    "          \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "              train_acc, loss.item()) +\n",
    "          \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "              val_acc, val_loss.item()))\n",
    "\n",
    "# plot training and validation accuracies over epochs\n",
    "plt.plot(range(1, n_epochs+1), train_acc_list, label='Training Accuracy')\n",
    "plt.plot(range(1, n_epochs+1), val_acc_list, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot training and validation losses over epochs\n",
    "plt.plot(range(1, n_epochs+1), train_loss_list, label='Training Loss')\n",
    "plt.plot(range(1, n_epochs+1), val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## after running for 14 weeks run them in LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, constraints\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define the LSTM model with output constraint\n",
    "class LSTM(keras.Model):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = layers.LSTM(hidden_size, input_shape=(None, input_size))\n",
    "        self.linear = layers.Dense(output_size, kernel_constraint=constraints.UnitNorm())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        lstm_out = self.lstm(inputs)\n",
    "        output = self.linear(lstm_out)\n",
    "        return output\n",
    "\n",
    "# set the hyperparameters\n",
    "input_size = 1\n",
    "hidden_size = 100\n",
    "output_size = 1\n",
    "lr = 0.001\n",
    "n_epochs = 100\n",
    "\n",
    "# create the LSTM model\n",
    "model = LSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "# define the loss function and optimizer\n",
    "criterion = keras.losses.MeanSquaredError()\n",
    "optimizer = optimizers.Adam(lr=lr)\n",
    "data =embdd\n",
    "x = data[:-1]\n",
    "y = data[1:]\n",
    "\n",
    "# split the data into training and validation sets\n",
    "train_size = int(len(x) * 0.8)\n",
    "train_x, train_y = x[:train_size], y[:train_size]\n",
    "val_x, val_y = x[train_size:], y[train_size:]\n",
    "\n",
    "# create the datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_x, val_y)).batch(32)\n",
    "\n",
    "# define the training and validation steps\n",
    "train_step = lambda inputs, labels: train_step_fn(model, inputs, labels, criterion, optimizer)\n",
    "val_step = lambda inputs, labels: val_step_fn(model, inputs, labels, criterion)\n",
    "\n",
    "@tf.function\n",
    "def train_step_fn(model, inputs, labels, criterion, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # forward pass\n",
    "        preds = model(inputs, training=True)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = criterion(labels, preds)\n",
    "\n",
    "    # compute the gradients and update the parameters\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def val_step_fn(model, inputs, labels, criterion):\n",
    "    # forward pass\n",
    "    preds = model(inputs, training=False)\n",
    "\n",
    "    # compute the loss\n",
    "    loss = criterion(labels, preds)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# train the model\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "for epoch in range(n_epochs):\n",
    "    # train the model\n",
    "    for train_x_batch, train_y_batch in train_ds:\n",
    "        train_loss = train_step(train_x_batch, train_y_batch)\n",
    "        train_loss_list.append(train_loss.numpy())\n",
    "\n",
    "    # evaluate the model on the validation set\n",
    "    for val_x_batch, val_y_batch in val_ds:\n",
    "        val_loss = val_step(val_x_batch, val_y_batch)\n",
    "        val_loss_list.append(val_loss.numpy())\n",
    "\n",
    "    # print the progress\n",
    "    print('Epoch [{}/{}], Train Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch+1, n_epochs, np.mean(train_loss_list), np.mean(val_loss_list)))\n",
    "\n",
    "# plot the losses\n",
    "plt.plot(range(1, len(train_loss_list)+1), train_loss_list, label='Training Loss')\n",
    "plt.plot(range(1, len(val_loss_list)+1), val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('3.9.2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8417d1dfa1d6fc0641ce57361e4c2424891a37ac2172e388282caca30e55c490"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
